# ğŸ›¡ï¸ Toxic Content Classification

A Deep Learning & NLP-based system for detecting and classifying toxic content into multiple categories.  
This project aims to help build safer online platforms by automatically identifying harmful text.

---

## ğŸ“Œ Overview

Toxic content detection is a Natural Language Processing (NLP) task used in:

- Social media moderation  
- Online communities  
- Comment filtering systems  
- Chat applications  

This project builds a **multi-class text classification model** to categorize text into **9 classes**, including:

- Safe  
- Unsafe  
- Violent Crimes  
- Non-Violent Crimes  
- Harassment  
- Hate Speech  
- Sexual Content  
- Unknown S-Type  
- Other Toxic Categories  

---

## ğŸ§  Models & Techniques

The project experiments with different NLP architectures:

- LSTM / BiLSTM  
- Transformer-based models (BERT family)  
- Pretrained word embeddings  
- Fine-tuning strategies  
- Class weighting for imbalanced data  

---

## ğŸ“‚ Project Structure

Toxic_Content_Classification/
â”‚
â”œâ”€â”€ data/ # Dataset files
â”œâ”€â”€ notebooks/ # Jupyter notebooks for experiments
â”œâ”€â”€ models/ # Saved trained models
â”œâ”€â”€ utils/ # Helper functions
â”œâ”€â”€ app/ # Deployment (Streamlit / API)
â”œâ”€â”€ train.py # Training script
â”œâ”€â”€ evaluate.py # Evaluation script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
